{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/home/ubuntu/Downloads/sentiment-analysis-on-movie-reviews/train.tsv', sep='\\t', header=0)\n",
    "test = pd.read_csv('/home/ubuntu/Downloads/sentiment-analysis-on-movie-reviews/test.tsv', sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((156060, 4), (66292, 3))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156061</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156062</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156063</td>\n",
       "      <td>8545</td>\n",
       "      <td>An</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156064</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156065</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase\n",
       "0    156061        8545  An intermittently pleasing but mostly routine ...\n",
       "1    156062        8545  An intermittently pleasing but mostly routine ...\n",
       "2    156063        8545                                                 An\n",
       "3    156064        8545  intermittently pleasing but mostly routine effort\n",
       "4    156065        8545         intermittently pleasing but mostly routine"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_docs_train = train['Phrase'].values\n",
    "raw_docs_test = test['Phrase'].values\n",
    "sentiment_train = train['Sentiment'].values\n",
    "num_labels = len(np.unique(sentiment_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.unique(sentiment_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preproceesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'how', 'll', 'doing', \"you'll\", 'couldn', 'have', 'under', 'her', 'but', 'hadn', 'yours', 'again', 'so', 'will', 'at', 'mustn', 'most', 'he', 'after', 'some', 'should', 'out', \"hadn't\", 'that', 'having', 'i', 'yourself', \"couldn't\", 'is', 'off', 'are', \"don't\", \"you're\", 'his', 'weren', \"you've\", 'because', 'between', 'to', \"weren't\", 'ain', \"shouldn't\", 'we', 'than', \"won't\", \"aren't\", 'no', 'why', 'through', 'won', 'needn', 'by', 'other', 'not', 'don', \"she's\", 'whom', 'more', 'in', \"needn't\", 'himself', 'these', 'about', 'any', 'nor', 'am', 'this', 'does', 'from', 'me', 'over', 'ourselves', \"wasn't\", 'or', 'was', 'for', 'ours', 'being', 'before', 'has', 'herself', 'such', \"hasn't\", 'an', 'below', 'our', 'do', 'where', 'further', 'did', 'with', 't', 'while', \"doesn't\", 'just', 'she', 'their', 'itself', 'both', 'only', 'doesn', 'shouldn', 's', 'you', 'it', 'ma', \"wouldn't\", 'now', 'own', \"didn't\", 'aren', 'haven', 'm', 'each', 'were', 'then', 'hasn', 'isn', 'didn', \"you'd\", 'who', 'if', \"isn't\", 'yourselves', 'themselves', 'above', 'wouldn', 'few', 'my', 'when', 'can', 'into', 'shan', 'them', 'of', 'your', 're', 'what', 'myself', 'had', 'during', 'be', 'there', 'until', 'those', 'which', 'a', \"haven't\", 'they', 'down', \"that'll\", 'once', \"mustn't\", 'wasn', 'same', \"should've\", \"shan't\", \"mightn't\", 'too', 'o', 'theirs', 'been', 'as', 'all', \"it's\", 'y', 'on', 've', 'him', 'here', 'mightn', 'and', 'up', 'd', 'hers', 'its', 'the', 'against', 'very'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "print (stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'how', 'll', 'doing', \"you'll\", 'couldn', 'have', 'under', 'her', 'but', 'hadn', 'yours', 'again', 'so', 'will', 'at', 'mustn', 'most', '}', 'he', 'after', 'some', 'should', 'out', \"hadn't\", 'that', '(', 'having', 'i', 'yourself', \"couldn't\", 'is', 'off', 'are', \"don't\", \"you're\", 'his', 'weren', \"you've\", 'because', 'between', 'to', \"weren't\", ',', 'ain', \"shouldn't\", 'we', 'than', \"won't\", \"aren't\", 'no', 'why', 'through', 'won', 'needn', 'by', 'other', 'not', '[', 'don', \"she's\", 'whom', 'more', 'in', \"needn't\", 'himself', 'these', 'about', '\"', 'any', 'nor', 'am', 'this', 'does', 'from', 'me', 'over', 'ourselves', \"wasn't\", 'or', 'was', 'for', 'ours', 'being', 'before', 'has', 'herself', 'such', \"hasn't\", 'an', 'below', '.', 'our', 'do', 'where', 'further', ']', 'did', 'with', 't', 'while', \"doesn't\", 'just', 'she', 'their', 'itself', 'both', 'only', 'doesn', 'shouldn', 's', 'you', 'it', 'ma', \"wouldn't\", 'now', 'own', \"didn't\", 'aren', 'haven', 'm', 'each', 'were', 'then', 'hasn', 'isn', 'didn', \"you'd\", 'who', 'if', \"isn't\", '{', 'yourselves', 'themselves', 'above', 'wouldn', 'few', 'my', \"'\", 'when', 'can', 'into', 'shan', 'them', ':', ')', 'of', 'your', 're', 'what', 'myself', 'had', 'during', 'be', 'there', 'until', 'those', 'which', 'a', \"haven't\", 'they', 'down', \"that'll\", 'once', \"mustn't\", 'wasn', 'same', \"should've\", \"shan't\", \"mightn't\", 'too', 'o', 'theirs', 'been', ';', 'as', 'all', \"it's\", 'y', 'on', 've', 'him', 'here', 'mightn', 'and', 'up', 'd', 'hers', 'its', 'the', 'against', 'very'}\n"
     ]
    }
   ],
   "source": [
    "stop_words.update(['.', ',', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}'])\n",
    "print (stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-processing train docs...\n",
      "\n",
      "\n",
      "A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .\n",
      "\n",
      "\n",
      "['A', 'series', 'of', 'escapades', 'demonstrating', 'the', 'adage', 'that', 'what', 'is', 'good', 'for', 'the', 'goose', 'is', 'also', 'good', 'for', 'the', 'gander', ',', 'some', 'of', 'which', 'occasionally', 'amuses', 'but', 'none', 'of', 'which', 'amounts', 'to', 'much', 'of', 'a', 'story', '.']\n",
      "\n",
      "\n",
      "['A', 'series', 'escapades', 'demonstrating', 'adage', 'good', 'goose', 'also', 'good', 'gander', 'occasionally', 'amuses', 'none', 'amounts', 'much', 'story']\n",
      "\n",
      "\n",
      "['a', 'seri', 'escapad', 'demonstr', 'adag', 'good', 'goos', 'also', 'good', 'gander', 'occasion', 'amus', 'none', 'amount', 'much', 'stori']\n"
     ]
    }
   ],
   "source": [
    "print (\"pre-processing train docs...\")\n",
    "processed_docs_train = []\n",
    "for index, doc in enumerate(raw_docs_train):\n",
    "    tokens = word_tokenize(doc)\n",
    "    filtered = [word for word in tokens if word not in stop_words]\n",
    "    stemmed = [stemmer.stem(word) for word in filtered]\n",
    "    processed_docs_train.append(stemmed)\n",
    "    \n",
    "    if index == 0:\n",
    "        print ('\\n')\n",
    "        print (doc)\n",
    "        print ('\\n')\n",
    "        print (tokens)\n",
    "        print ('\\n')\n",
    "        print (filtered)\n",
    "        print ('\\n')\n",
    "        print (stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-processing test docs...\n"
     ]
    }
   ],
   "source": [
    "print (\"pre-processing test docs...\")\n",
    "processed_docs_test = []\n",
    "for doc in raw_docs_test:\n",
    "    tokens = word_tokenize(doc)\n",
    "    filtered = [word for word in tokens if word not in stop_words]\n",
    "    stemmed = [stemmer.stem(word) for word in filtered]\n",
    "    processed_docs_test.append(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs_all = np.concatenate((processed_docs_train, processed_docs_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary size:  13758\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(processed_docs_all)\n",
    "dictionary_size = len(dictionary.keys())\n",
    "print (\"dictionary size: \", dictionary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a', 'stori')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[0], dictionary[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting to token ids...\n",
      "['a', 'seri', 'escapad', 'demonstr', 'adag', 'good', 'goos', 'also', 'good', 'gander', 'occasion', 'amus', 'none', 'amount', 'much', 'stori']\n",
      "[0, 13, 6, 5, 1, 8, 9, 2, 8, 7, 12, 4, 11, 3, 10, 14]\n",
      "[[0, 13, 6, 5, 1, 8, 9, 2, 8, 7, 12, 4, 11, 3, 10, 14]]\n",
      "[16]\n"
     ]
    }
   ],
   "source": [
    "print (\"converting to token ids...\")\n",
    "word_id_train, word_id_len = [], []\n",
    "for index,doc in enumerate(processed_docs_train):\n",
    "    word_ids = [dictionary.token2id[word] for word in doc]\n",
    "    word_id_train.append(word_ids)\n",
    "    word_id_len.append(len(word_ids))\n",
    "    \n",
    "    if index == 0:\n",
    "        print (doc)\n",
    "        print (word_ids)\n",
    "        print (word_id_train)\n",
    "        print (word_id_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_id_test, word_ids = [], []\n",
    "for doc in processed_docs_test:\n",
    "    word_ids = [dictionary.token2id[word] for word in doc]\n",
    "    word_id_test.append(word_ids)\n",
    "    word_id_len.append(len(word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.169919766856156\n",
      "3.8047838578714424\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "seq_len = np.round((np.mean(word_id_len) + 2*np.std(word_id_len))).astype(int)\n",
    "print (np.mean(word_id_len))\n",
    "print (np.std(word_id_len))\n",
    "print (seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad sequences\n",
    "word_id_train = sequence.pad_sequences(np.array(word_id_train), maxlen=seq_len)\n",
    "word_id_test = sequence.pad_sequences(np.array(word_id_test), maxlen=seq_len)\n",
    "y_train_enc = np_utils.to_categorical(sentiment_train, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    1     8     9 ...     3    10    14]\n",
      " [    0     0     0 ...     1     8     9]\n",
      " [    0     0     0 ...     0     0    13]\n",
      " ...\n",
      " [    0     0     0 ...     0 11848 11849]\n",
      " [    0     0     0 ...     0     0 11848]\n",
      " [    0     0     0 ...     0     0 11849]]\n"
     ]
    }
   ],
   "source": [
    "print (word_id_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print (y_train_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting LSTM ...\n"
     ]
    }
   ],
   "source": [
    "#LSTM\n",
    "print (\"fitting LSTM ...\")\n",
    "model = Sequential()\n",
    "model.add(Embedding(dictionary_size, 128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "156060/156060 [==============================] - 49s 315us/step - loss: 0.9977 - acc: 0.5989\n",
      "Epoch 2/3\n",
      "156060/156060 [==============================] - 49s 315us/step - loss: 0.8259 - acc: 0.6612\n",
      "Epoch 3/3\n",
      "156060/156060 [==============================] - 49s 316us/step - loss: 0.7704 - acc: 0.6790\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f366d607ac8>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(word_id_train, y_train_enc, epochs=3, batch_size=256, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict_classes(word_id_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a submission\n",
    "test['Sentiment'] = test_pred.reshape(-1,1) \n",
    "header = ['PhraseId', 'Sentiment']\n",
    "test.to_csv('./final_lstm_cnn.csv', columns=header, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
